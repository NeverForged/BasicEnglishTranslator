{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic English Translation and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a dictionary to save my basic english translation of words as a list, where position 0 is the word to go to, and 1 -> end are other related words, generally the single word that matched to get us to a word, in order to apply graph theory to my model performance (likey to see what codes to the basic words in Ogden's Basic English)\n",
    "\n",
    "Will also save all things I analyse to a single model in gensim, and will switch to it once it is larger than google-news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darin\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import cPickle as pickle\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import gensim as gensim\n",
    "import string\n",
    "import re\n",
    "import nltk.data\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert basic english words to a list\n",
    "basic_english_df = pd.read_csv('data/basic_english_wordlist.csv')\n",
    "basic_english = [a for a in basic_english_df['WORD']]\n",
    "# add the various conjugations of 'to be' and 'a'\n",
    "basic_english.append('an')\n",
    "basic_english.append('is')\n",
    "basic_english.append('was')\n",
    "basic_english.append('are')\n",
    "basic_english.append('were')\n",
    "basic_english.append('they')\n",
    "basic_english[350] = 'big' # 'I' causing weird issues...\n",
    "basic_english.append('she')\n",
    "basic_english.append('hers')\n",
    "basic_english.append('his')\n",
    "basic_english.append('my')\n",
    "basic_english.append('him')\n",
    "basic_english.append('her')\n",
    "basic_english.append('your')\n",
    "basic_english.append('their')\n",
    "basic_english.append('might')\n",
    "basic_english.append('must')\n",
    "basic_english.append('can')\n",
    "basic_english.append('did')\n",
    "basic_english.append('could')\n",
    "basic_english.append('should')\n",
    "basic_english.append('would')\n",
    "basic_english.append('that')\n",
    "basic_english.append('what')\n",
    "basic_english.append('we')\n",
    "basic_english.append('small')\n",
    "basic_english[basic_english.index('colour')] = 'color'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darin\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# adding contractions...\n",
    "contractions_df = pd.read_csv('data/contractions.csv', sep=' -')\n",
    "contractions = [word for word in contractions_df['from']]\n",
    "contractions[18] = \"mightn't\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.30s\n"
     ]
    }
   ],
   "source": [
    "start = time.clock()\n",
    "Google_model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "print '{:.2f}s'.format(time.clock() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n"
     ]
    }
   ],
   "source": [
    "vocab_google = Google_model.vocab.keys()\n",
    "print len(vocab_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    my_dict = pickle.load(open('data/basic_english.pickle', \"rb\" ))\n",
    "except:\n",
    "    st = LancasterStemmer()\n",
    "    stem_gn = [st.stem(key) for key in Google_model.vocab.keys()]\n",
    "    stem_se = [st.stem(word) for word in basic_english]\n",
    "    print 'No saved dictionary...'\n",
    "    my_dict = {}\n",
    "    threshold = 0.25\n",
    "    for sim_in in xrange(len(basic_english)-1, 0, -1):\n",
    "        print\n",
    "        print basic_english[sim_in]\n",
    "        print '**'*8\n",
    "        indices = [i for i, s in enumerate(stem_gn) if stem_se[sim_in] == s]\n",
    "        check = [i for i, s in enumerate(vocab_google) if basic_english[sim_in] == s]\n",
    "        #print check, indices\n",
    "        if len(check) > 0:\n",
    "            for index in indices: \n",
    "                if Google_model.similarity(basic_english[sim_in], vocab_google[index]) >= threshold:\n",
    "                    print '{} -> {}'.format(vocab_google[index], Google_model.similarity(basic_english[sim_in], vocab_google[index]))\n",
    "                    my_dict[vocab_google[index].lower()] = [vocab_google[index].lower(), basic_english[sim_in].lower()]\n",
    "        my_dict[basic_english[sim_in].lower()] = [basic_english[sim_in].lower(), basic_english[sim_in].lower()]\n",
    "        \n",
    "    my_dict['i'] = ['i','i'] # add 'I\n",
    "    basic_english.append('i')\n",
    "    for word in basic_english:\n",
    "        wordy = word\n",
    "        if len(word) <= 1:\n",
    "            wordy = word+\"'\"\n",
    "        for con in contractions:\n",
    "            if wordy.lower() in con.lower()[0:len(wordy)]:\n",
    "                my_dict[con.lower()] = [con.lower(), word.lower()]\n",
    "    my_dict[\"am\"] = ['am','am']\n",
    "    my_dict[\"a\"] = ['a','a']\n",
    "#     with open('data/basic_english.pickle', 'wb') as handle:\n",
    "#          pickle.dump(my_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hold_dict = my_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time as time\n",
    "import sys \n",
    "from nltk import pos_tag, word_tokenize\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "def make_simple_english(input_text, threshold=0, dictionary=hold_dict, save_bypass=False):\n",
    "    '''\n",
    "    Return the input_text translated into simple english\n",
    "    Input: String\n",
    "    Output: String\n",
    "    '''\n",
    "    temp_dict = {}\n",
    "    temp_dict = hold_dict.copy()\n",
    "    if threshold == 0:\n",
    "        threshold = 60.0/sqrt(len(input_text))\n",
    "    done = 0\n",
    "    # timer...\n",
    "    start= time.clock()\n",
    "    input_text = input_text.replace('—',' - ').replace(\"’\",\" ' \")\n",
    "    input_text = ''.join([a if ord(a) < 128 else '' for a in list(input_text)])\n",
    "    words = pos_tag(word_tokenize(input_text)) # makes a list of words...\n",
    "\n",
    "    # These simply pass thru the model\n",
    "    pass_thru = ['CD', # CD: numeral, cardinal\n",
    "                 'EX', # EX: existential there\n",
    "                 'FW', # FW: foreign word\n",
    "                 'LS', # LS: list item marker\n",
    "                 'NNP', # NNP: noun, proper, singular\n",
    "                 'NNPS', # NNPS: noun, proper, plural\n",
    "                 'PRP', # PRP: pronoun, personal\n",
    "                 'SYM', # SYM: symbol\n",
    "                 'TO', # TO: \"to\" as preposition or infinitive marker\n",
    "                 'POS',\n",
    "                 '$', # $: dollar\n",
    "                 '(',\n",
    "                 ')',\n",
    "                 ',',\n",
    "                 '.',\n",
    "                 ':',\n",
    "                 '\"'\n",
    "                ] \n",
    "    # make these Basic\n",
    "    make_simple = ['CC', # CC: conjunction, coordinating\n",
    "                   'DT', # DT: determiner\n",
    "                   'IN', # IN: preposition or conjunction, subordinating\n",
    "                   'JJ', # JJ: adjective or numeral, ordinal\n",
    "                   'JJR', # JJR: adjective, comparative\n",
    "                   'JJS', # JJR: adjective, comparative\n",
    "                   'MD', # MD: modal auxiliary\n",
    "                   'NN', # NN: noun, common, singular or mass\n",
    "                   'NNS', # NNS: noun, common, plural\n",
    "                   'PDT', # PDT: pre-determiner\n",
    "                   'PDT', # PDT: pre-determiner\n",
    "                   'PRP$', # PRP$: pronoun, possessive\n",
    "                   'RB', # RB: adverb\n",
    "                   'RBR', # RBR: adverb, comparative\n",
    "                   'RBS', # RBS: adverb, superlative\n",
    "                   'RP', # RP: particle\n",
    "                   'UH', # UH: interjection\n",
    "                   'VB', # VB: verb, base form\n",
    "                   'VBD', # VBD: verb, past tense\n",
    "                   'VBG', # VBG: verb, present participle or gerund\n",
    "                   'VBN', # VBN: verb, past participle\n",
    "                   'VBP', # VBP: verb, present tense, not 3rd person singular\n",
    "                   'VBZ', # VBZ: verb, present tense, 3rd person singular\n",
    "                   'WDT', # WDT: WH-determiner\n",
    "                   'WP', # WP: WH-pronoun\n",
    "                   'WP$', # WP$: WH-pronoun, possessive\n",
    "                   'WRB' #WRB: Wh-adverb\n",
    "                  ]\n",
    "    done == 0\n",
    "    count_replacements = 0\n",
    "    lst_ret = []\n",
    "    for word in words:\n",
    "        if word[1] in pass_thru:\n",
    "            # put it in and move on... it's proper or whatever\n",
    "            lst_ret.append(word[0])\n",
    "        else:\n",
    "            # We have a word we need to replace...\n",
    "            clean = word[0].strip(string.punctuation).lower() # bath it...\n",
    "            # ...and bring it to the function\n",
    "            if clean in temp_dict.keys():  # already simple... throw it in and move on\n",
    "                lst_ret.append(retain_capitalization(temp_dict[clean][0], word[0]))\n",
    "            elif clean != '': # not alread simply/basic...\n",
    "                start_this = time.clock() # timing for testing\n",
    "                try: # in case it fails...\n",
    "                    lst = list(set(Google_model.most_similar(clean)))\n",
    "                    done = 0\n",
    "                    n = 0\n",
    "                    while done == 0:\n",
    "                        check = list(lst)[n][0]\n",
    "                        n +=1\n",
    "                        check_clean = check.strip(string.punctuation).lower()\n",
    "                        if check_clean in temp_dict.keys():\n",
    "                            done = 1\n",
    "                            # add to dictionary...based on what's there, retaining grouping info\n",
    "                            temp_dict[clean] = [temp_dict[check_clean][0], check_clean]\n",
    "                            if save_bypass:\n",
    "                                my_dict[clean.lower()] = [temp_dict[check_clean][0].lower(), check_clean.lower()]\n",
    "                            # add to lst\n",
    "                            lst_ret.append(retain_capitalization(temp_dict[clean][0], word[0]))\n",
    "                            print \"     {}: {} -> {} ({}s) {}\".format(word, clean, temp_dict[check_clean][0].lower(), time.clock()-start_this, n)\n",
    "                        else:\n",
    "                            # add all similar words to that to the lst\n",
    "                            if time.clock() - start_this < threshold:\n",
    "                                [lst.append(a) for a in Google_model.most_similar(check, topn=3) if a not in lst]\n",
    "                            else: # timeout!\n",
    "                                done = 1\n",
    "                                temp_dict[clean] = [clean.lower(), clean.lower()]\n",
    "                                lst_ret.append(retain_capitalization(temp_dict[clean][0], word[0]))\n",
    "                                # print \"     {}: {} -> {} ({}s) {}\".format(word, clean.lower(),  temp_dict[clean][0], time.clock()-start_this, n)         \n",
    "                                # timeouts = add if training off simple wikipedia\n",
    "                                if save_bypass:\n",
    "                                    my_dict[clean] = [clean.lower(), clean.lower()]\n",
    "                except:\n",
    "                    lst_ret.append(retain_capitalization(word[0], word[0]))\n",
    "                    temp_dict[word[0].lower()] = word[0].lower()\n",
    "                    # print \"     >{}: {} [->] {} ({}s)\".format(word, clean, word[0], time.clock()-start_this)\n",
    "\n",
    "    end = time.clock()\n",
    "    print 'Time: {:.2f}s'.format(end-start)\n",
    "    txt = replace_punctuation(' '.join(lst_ret))\n",
    "    txt = txt.encode('utf-8')\n",
    "    txt = re.sub(\"\\xe2\\x80\\x93\", \"-\", txt)\n",
    "    return txt\n",
    "\n",
    "\n",
    "def retain_capitalization(new_word, original_word):\n",
    "    '''\n",
    "    Checks the original_word for capitalization, if it has it, capitalizes the frst letter\n",
    "    of new_word, returns new_word.\n",
    "    '''\n",
    "    if original_word[0] in list('ABCDEFGHIJKLMNOPQRSTUVWXYZ'):\n",
    "        lst = list(new_word)\n",
    "        lst[0] = lst[0].upper()\n",
    "        new_word = ''.join(lst)\n",
    "    return new_word\n",
    "\n",
    "\n",
    "def replace_punctuation(text):\n",
    "    '''\n",
    "    Tokenizing takes the punctuation as it's own item in the list.\n",
    "    This takes the created string and replaces all 'end ?' with 'end?'\n",
    "    '''\n",
    "    text = text.replace(' .','.')\n",
    "    text = text.replace(' ?','?')\n",
    "    text = text.replace(' !','!')\n",
    "    text = text.replace(' ,',',')\n",
    "    text = text.replace(' ;',';')\n",
    "    text = text.replace(' \"','\"')\n",
    "    text = text.replace(\" '\",\"'\")\n",
    "    text = text.replace('( ','(')\n",
    "    text = text.replace(' )',')')\n",
    "    text = text.replace('$ ','$')\n",
    "    text = text.replace(' *','*')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horses are mammals of the family Equidae. The common horse is the species Equus caballus. It was domesticated from wild horses by humans at least 5000 years ago. They are large, strong animals, and some breeds are used to pull heavy loads. Racehorses can gallop up to 30 miles an hour.\n",
      "A male horse is a stallion, and a female horse is a mare. The general term for a young horse is foal. A young female horse is a filly, and a young male horse is a colt. A castrated horse is a gelding. Horses have hooves which need protection by horseshoes from hard or rough ground.\n",
      "\n",
      "\n",
      "The evolution of horses has been well studied.[1][2] Fifty million years ago, there were no horses as we know them now. Of the earliest fossil horse, the North American one is called Eohippus, and the Eurasian one is called Hyracotherium. Both were small animals: Eohippus was the larger of the two at twice the size of a terrier dog.\n",
      "Many changes took place between those little animals and today's horse.[3] These changes are best explained as adaptations to its changing ecological niche. From a small forest-dweller eating nuts and fruit to a larger forest browser eating leaves and small branches. Finally, the modern horse is a grazer on open grassland, with different teeth, legs for running and much larger size. Major changes happened in the mid-Miocene when the climate became cooler, and grassland began to replace forests. This change continued, and several groups of mammals changed from browsers to grazers.[1][2]\n",
      "Horses have been domesticated for at least 5000 years.[4] They have been used by humans in many different ways for travel, work, food, and pleasure. Cavalry horses Were used in war until the middle 20th century. Some people keep horses as pets.\n",
      "They are used for riding, as transport, racing, and just plain riding. They are also used for carrying things or pulling carts, or to help plow farmer's fields in agriculture.. Today, horses are mostly used for entertainment and sports. They are also still used for work and transportation in some places.\n",
      "\"Equus\" is the old Latin word for horse.\n",
      "Horses are used in equestrianism, which is equine sports such as cross-country, showjumping, dressage, horse polo, rodeo events etc. Showjumping, cross-country and dressage are Olympic sports.\n",
      "Horsehide is a tough leather made from the skin of horses. Horsehair is used to make a stiff fabric. Horsehair can also be used as a stuffing for furniture. Horsehair can be mixed with plaster to make it strong. Horse bones can be used to make gelatin for food. The bones can also be used to make glue. Animal glue is still preferred by some wood workers.[5]\n",
      "Horses are used all over the world to carry people and pull carts. They are used in big cities to help police watch and protect people in crowds.[6]\n",
      "These are some well-known horse breeds:\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "r = requests.get('https://simple.wikipedia.org/wiki/Horse')\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "a = 0\n",
    "ret = ''\n",
    "\n",
    "\n",
    "tags = soup.find_all('p')\n",
    "MyText = '\\n'.join([tag.get_text() for tag in tags])\n",
    "\n",
    "\n",
    "# print tags[tags.index('title=')+6:tags.index('/>')]\n",
    "#tags.span.clear()\n",
    "print MyText\n",
    "#print tags[tags.index('class')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hold_dict = my_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to split a book into parsed sentences\n",
    "def book_to_sentences(input_text, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(input_text.encode(\"ascii\",\"replace\").strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( book_to_wordlist( raw_sentence, remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences\n",
    "\n",
    "def book_to_wordlist(book_text, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    #  \n",
    "    #  Decode from UTF-8\n",
    "#     tbl = dict.fromkeys(i for i in xrange(sys.maxunicode)\n",
    "#               if unicodedata.category(unichr(i)).startswith('P'))\n",
    "#     book_text = book.text.translate(tbl)\n",
    "\n",
    "    #\n",
    "    # 3. Convert words to lower case and split themstring.decode('utf-8')\n",
    "    words = book_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "features = vectorizer.fit_transform([MyText])\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "features = features.toarray()\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading existant set of sentences\n",
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "# Load in sentences\n",
    "print 'loading existant set of sentences'\n",
    "try:\n",
    "    sentences = pickle.load(open('data/sentences.pickle', \"rb\" ))\n",
    "except:\n",
    "    print 'load failed'\n",
    "    sentences = []  # Initialize an empty list of sentences\n",
    "print \"Parsing sentences from training set\"\n",
    "\n",
    "    \n",
    "MyText = MyText.encode('ascii', 'replace')\n",
    "sentences += book_to_sentences(MyText, tokenizer)\n",
    "\n",
    "with open('data/sentences.pickle', 'wb') as handle:\n",
    "     pickle.dump(sentences, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-29 08:59:46,470 : INFO : collecting all words and their counts\n",
      "2017-06-29 08:59:46,470 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-06-29 08:59:46,479 : INFO : collected 5144 word types from a corpus of 24730 raw words and 1699 sentences\n",
      "2017-06-29 08:59:46,480 : INFO : Loading a fresh vocabulary\n",
      "2017-06-29 08:59:46,490 : INFO : min_count=4 retains 940 unique words (18% of original 5144, drops 4204)\n",
      "2017-06-29 08:59:46,493 : INFO : min_count=4 leaves 18453 word corpus (74% of original 24730, drops 6277)\n",
      "2017-06-29 08:59:46,502 : INFO : deleting the raw counts dictionary of 5144 items\n",
      "2017-06-29 08:59:46,506 : INFO : sample=0.001 downsamples 53 most-common words\n",
      "2017-06-29 08:59:46,509 : INFO : downsampling leaves estimated 11945 word corpus (64.7% of prior 18453)\n",
      "2017-06-29 08:59:46,515 : INFO : estimated required memory for 940 words and 300 dimensions: 2726000 bytes\n",
      "2017-06-29 08:59:46,520 : INFO : resetting layer weights\n",
      "2017-06-29 08:59:46,543 : INFO : training model with 4 workers on 940 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-06-29 08:59:46,635 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-06-29 08:59:46,638 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-06-29 08:59:46,642 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-06-29 08:59:46,651 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-06-29 08:59:46,653 : INFO : training on 123650 raw words (59693 effective words) took 0.1s, 574194 effective words/s\n",
      "2017-06-29 08:59:46,655 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-06-29 08:59:46,660 : INFO : saving Word2Vec object under data/all_parsed300features_4min_word_count_10context.npy, separately None\n",
      "2017-06-29 08:59:46,661 : INFO : not storing attribute syn0norm\n",
      "2017-06-29 08:59:46,664 : INFO : not storing attribute cum_table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-29 08:59:46,687 : INFO : saved data/all_parsed300features_4min_word_count_10context.npy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22s\n",
      "940\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "start = time.clock()\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 4   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model_name = 'all_parsed'+str(num_features)+'features_'+str(min_word_count)+'min_word_count_'+str(context)+'context.npy'\n",
    "\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \n",
    "                          size=num_features, min_count = min_word_count, \n",
    "                          window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "# model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "\n",
    "model.save('data/'+model_name)\n",
    "print '{:.2f}s'.format(time.clock() - start)\n",
    "print len(model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ('mammals', 'NNS'): mammals -> species (5.98688278098s) 7\n",
      "     ('domesticated', 'VBN'): domesticated -> animals (8.85932698154s) 11\n",
      "     ('heavy', 'JJ'): heavy -> heavier (0.790520347037s) 1\n",
      "     ('loads', 'NNS'): loads -> loading (3.99446045101s) 5\n",
      "     ('Racehorses', 'NNS'): racehorses -> horses (1.59437440667s) 2\n",
      "     ('miles', 'NNS'): miles -> km (5.52157133246s) 7\n",
      "     ('stallion', 'NN'): stallion -> horse (3.25855139727s) 4\n",
      "     ('mare', 'NN'): mare -> horse (0.819043770022s) 1\n",
      "     ('foal', 'VBN'): foal -> horse (1.59054428775s) 2\n",
      "     ('filly', 'RB'): filly -> horse (6.42700235812s) 8\n",
      "     ('colt', 'NN'): colt -> horse (6.48087844242s) 8\n",
      "     ('gelding', 'NN'): gelding -> horse (6.38222492971s) 8\n",
      "     ('adaptations', 'NNS'): adaptations -> adapted (5.61576065293s) 7\n",
      "     ('ecological', 'JJ'): ecological -> ecology (2.37484044463s) 3\n",
      "     ('leaves', 'NNS'): leaves -> sends (2.39878194712s) 3\n",
      "     ('grazer', 'NN'): grazer -> insectivore (8.10579108449s) 10\n",
      "     ('cooler', 'NN'): cooler -> colder (2.39088625278s) 3\n",
      "     ('replace', 'VB'): replace -> replaced (5.60945547521s) 7\n",
      "     ('forests', 'NNS'): forests -> forest (1.58234163081s) 2\n",
      "     ('browsers', 'NNS'): browsers -> browser (7.20058287338s) 9\n",
      "     ('grazers', 'NNS'): grazers -> insectivores (1.62058474597s) 2\n",
      "     ('pets', 'NNS'): pets -> dogs (2.39862036707s) 3\n",
      "     ('racing', 'NN'): racing -> races (6.53765862203s) 8\n",
      "     ('plain', 'VB'): plain -> simple (3.19645250791s) 4\n",
      "     ('carrying', 'VBG'): carrying -> transporting (2.39607301206s) 3\n",
      "     ('agriculture..', 'NN'): agriculture -> farming (0.792865430678s) 1\n",
      "     ('equestrianism', 'NN'): equestrianism -> eventing (4.82660642773s) 6\n",
      "     ('equine', 'JJ'): equine -> horses (0.80197001248s) 1\n",
      "     ('showjumping', 'VBG'): showjumping -> eventing (4.83270143285s) 6\n",
      "     ('dressage', 'NN'): dressage -> eventing (1.60123859606s) 2\n",
      "     ('tough', 'JJ'): tough -> harder (2.39605088863s) 3\n",
      "     ('fabric', 'NN'): fabric -> cloth (2.4176480976s) 3\n",
      "     ('Horsehair', 'NN'): horsehair -> leather (8.78783076998s) 11\n",
      "     ('plaster', 'NN'): plaster -> wax (4.01342852729s) 5\n",
      "     ('gelatin', 'NN'): gelatin -> powdered (6.39025573501s) 8\n",
      "     ('preferred', 'VBN'): preferred -> choose (2.4876071258s) 3\n",
      "     ('crowds', 'NN'): crowds -> crowd (1.59608976266s) 2\n",
      "Time: 352.74s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Horses are species of the family Equidae. The common horse is the species Equus caballus. It was animals from wild horses by humans at least 5000 years earlier. They are large, strong animals, and some breeds are used to pull heavier loading. Horses can gallop up to 30 km an hour. A male horse is a horse, and a female horse is a horse. The general term for a young horse is horse. A young female horse is a horse, and a young male horse is a horse. A castrated horse is a horse. Horses have hooves which need protection by horseshoes from hard or rough ground. The evolution of horses has been well studied. [ 1 ] 2 Fifty million years earlier, there were no horses as we know them now. Of the earliest fossil horse, the North American one is called Eohippus, and the Eurasian one is called Hyracotherium. Both were small animals : Eohippus was the larger of the two at twice the size of a terrier dog. Many changes took place between those little animals and today's horse. 3 These changes are best explained as adapted to its changing ecology niche. From a small forest-dweller eating nuts and fruit to a larger forest browser eating sends and small branches. Finally, the modern horse is a insectivore on open grassland, with different teeth, legs for running and much larger size. Major changes happened in the mid-Miocene when the climate became colder, and grassland started to replaced forest. This change continued, and many groups of species changed from browser to insectivores. [ 1 ] 2 Horses have been animals for at least 5000 years. [ 4 They have been used by humans in many different ways for travel, work, food, and pleasure. Cavalry horses Were used in war until the middle 20th century. Some people keep horses as dogs. They are used for riding, as transport, races, and just simple riding. They are also used for transporting things or pulling carts, or to help plow farmer's fields in farming Today, horses are mostly used for entertainment and sports. They are also still used for work and transportation in some places. Equus is the old Latin word for horse. Horses are used in eventing, which is horses sports such as cross-country, eventing, eventing, horse polo, rodeo events etc. Eventing, c and eventing are Olympic sports. Horsehide is a harder leather made from the skin of horses. Horsehair is used to make a stiff cloth. Horsehair can also be used as a stuffing for furniture. Leather can be mixed with wax to make it strong. Horse bones can be used to make powdered for food. The bones can also be used to make glue. Animal glue is still choose by some wood workers. [ 5 Horses are used all over the world to carry people and pull carts. They are used in big cities to help police watch and protect people in crowd. 6 These are some well-known horse breeds :\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_simple_english(MyText, threshold=10.0, dictionary=my_dict, save_bypass=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_dict['adaptations'] = ['adapted', u'adapted']\n",
      "my_dict['replace'] = ['replaced', u'replaced']\n",
      "my_dict['fabric'] = ['cloth', u'cloth']\n",
      "my_dict['loads'] = ['loading', u'loading']\n",
      "my_dict['filly'] = ['horse', u'mare']\n",
      "my_dict['gelding'] = ['horse', u'colt']\n",
      "my_dict['mammals'] = ['species', u'species']\n",
      "my_dict['tough'] = [u'harder', u'harder']\n",
      "my_dict['grazers'] = [u'insectivores', u'insectivores']\n",
      "my_dict['carrying'] = [u'transporting', u'transporting']\n",
      "my_dict['racehorses'] = [u'horses', u'horses']\n",
      "my_dict['agriculture'] = [u'farming', u'farming']\n",
      "my_dict['preferred'] = [u'choose', u'prefer']\n",
      "my_dict['racing'] = ['races', u'races']\n",
      "my_dict['horsehair'] = ['leather', u'leather']\n",
      "my_dict['cooler'] = [u'colder', u'colder']\n",
      "my_dict['colt'] = ['horse', u'mare']\n",
      "my_dict['dressage'] = [u'eventing', u'eventing']\n",
      "my_dict['foal'] = ['horse', u'stallion']\n",
      "my_dict['heavy'] = ['heavier', u'heavier']\n",
      "my_dict['equestrianism'] = [u'eventing', u'eventing']\n",
      "my_dict['forests'] = ['forest', u'forest']\n",
      "my_dict['showjumping'] = [u'eventing', u'eventing']\n",
      "my_dict['pets'] = [u'dogs', u'dogs']\n",
      "my_dict['stallion'] = ['horse', u'horse']\n",
      "my_dict['gelatin'] = [u'powdered', u'powdered']\n",
      "my_dict['miles'] = ['km', u'km']\n",
      "my_dict['mare'] = ['horse', u'stallion']\n",
      "my_dict['browsers'] = ['browser', u'browser']\n",
      "my_dict['crowds'] = ['crowd', u'crowd']\n",
      "my_dict['plain'] = ['simple', u'simple']\n",
      "my_dict['ecological'] = ['ecology', u'ecology']\n",
      "my_dict['leaves'] = [u'sends', u'sends']\n",
      "my_dict['plaster'] = ['wax', u'wax']\n",
      "my_dict['equine'] = [u'horses', u'horses']\n",
      "my_dict['grazer'] = [u'insectivore', u'insectivore']\n",
      "my_dict['domesticated'] = [u'animals', u'animals']\n"
     ]
    }
   ],
   "source": [
    "hold = hold_dict.keys()\n",
    "my = my_dict.keys()\n",
    "for key in my_dict.keys():\n",
    "    if key not in hold and key != my_dict[key][0]:\n",
    "        print \"my_dict['{}'] = {}\".format(key, my_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dict['adaptations'] = ['adaptations', u'adapt']\n",
    "my_dict['replace'] = ['replace', u'replace']\n",
    "my_dict['fabric'] = ['cloth', u'cloth']\n",
    "my_dict['loads'] = ['loads', u'loading']\n",
    "my_dict['filly'] = ['horse', u'mare']\n",
    "my_dict['gelding'] = ['horse', u'colt']\n",
    "my_dict['mammals'] = ['mammals', u'mammal']\n",
    "my_dict['tough'] = [u'tough', u'tough']\n",
    "my_dict['grazers'] = [u'herbavores', u'herbavore']\n",
    "my_dict['carrying'] = [u'carrying', u'carry']\n",
    "my_dict['racehorses'] = [u'horses', u'horses']\n",
    "my_dict['agriculture'] = [u'farming', u'farming']\n",
    "my_dict['preferred'] = [u'preferred', u'prefer']\n",
    "my_dict['racing'] = ['racing', u'races']\n",
    "my_dict['horsehair'] = ['hair', u'hair']\n",
    "my_dict['cooler'] = [u'colder', u'cold']\n",
    "my_dict['colt'] = ['horse', u'mare']\n",
    "del my_dict['dressage']\n",
    "my_dict['foal'] = ['horse', u'stallion']\n",
    "my_dict['heavy'] = ['heavy', u'heavier']\n",
    "my_dict['equestrianism'] = [u'', u'eventing']\n",
    "my_dict['forests'] = ['woods', u'woods']\n",
    "my_dict['showjumping'] = [u'jumping', u'jump']\n",
    "my_dict['pets'] = [u'pets', u'pet']\n",
    "my_dict['stallion'] = ['horse', u'horse']\n",
    "my_dict['gelatin'] = [u'gelatin', u'gelatin']\n",
    "my_dict['miles'] = ['miles', u'mile']\n",
    "my_dict['mare'] = ['horse', u'stallion']\n",
    "my_dict['browsers'] = ['browsers', u'browser']\n",
    "my_dict['crowds'] = ['crowds', u'crowd']\n",
    "my_dict['plain'] = ['simple', u'simple']\n",
    "my_dict['ecological'] = ['ecological', u'ecology']\n",
    "my_dict['leaves'] = [u'leave', u'leave']\n",
    "my_dict['plaster'] = ['plaster', u'plaster']\n",
    "my_dict['equine'] = [u'horse-like', u'horses']\n",
    "my_dict['grazer'] = [u'herbavore', u'herbavore']\n",
    "my_dict['domesticated'] = [u'tamed', u'tame']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_dict['currency'] = ['money', u'money']\n",
      "my_dict['commemorate'] = ['celebrate', u'celebrate']\n",
      "my_dict['foundation'] = ['foundation', 'foundation']\n",
      "my_dict['nomadic'] = ['nomadic', 'nomadic']\n",
      "my_dict['enormous'] = ['large', u'massive']\n",
      "my_dict['kingdom'] = [u'kingdom', u'kingdom']\n",
      "my_dict['cut'] = ['cut', u'cut']\n",
      "my_dict['collectivization'] = ['communism', u'communism']\n",
      "my_dict['resulted'] = [u'resulted', u'result']\n",
      "my_dict['alone'] = ['alone', u'lone']\n",
      "my_dict['slashed'] = ['cut', u'cut']\n",
      "my_dict['technology'] = [u'technology', u'technologies']\n",
      "my_dict['accept'] = ['accept', u'accepted']\n",
      "my_dict['retained'] = ['held', u'retains']\n",
      "my_dict['bumper'] = ['bumper', 'bumper']\n",
      "my_dict['northeastern'] = ['northeastern', u'eastern']\n",
      "my_dict['races'] = ['races', u'race']\n",
      "my_dict['popularized'] = [u'made popular', u'popular']\n",
      "my_dict['nowadays'] = ['today', u'today']\n",
      "my_dict['decorations'] = [u'decorations', u'decoration']\n",
      "my_dict['underwent'] = ['underwent', 'underwent']\n",
      "my_dict['renowned'] = ['renowned', 'renowned']\n",
      "my_dict['blocking'] = [u'stopping', u'stop']\n",
      "my_dict['appreciate'] = ['enjoy', u'enjoy']\n",
      "my_dict['festival'] = ['festival', u'festivals']\n",
      "my_dict['incorporated'] = ['incorporated', 'incorporated']\n",
      "my_dict['helpless'] = ['helpless', 'helpless']\n",
      "my_dict['symbolizes'] = [u'symbolizes', u'symbol']\n",
      "my_dict['famine'] = ['famine', 'famine']\n",
      "my_dict['actually'] = ['really', u'real']\n",
      "my_dict['unresolved'] = ['unresolved', 'unresolved']\n",
      "my_dict['derived'] = ['derived', u'derives']\n",
      "my_dict['blossom'] = [u'flower', u'flowering']\n",
      "my_dict['celebrated'] = ['celebrated', u'celebrate']\n",
      "my_dict['isolationist'] = ['isolationist', 'isolationist']\n",
      "my_dict['civilization'] = ['humanity', u'mankind']\n",
      "my_dict['slash'] = ['cut', u'cut']\n",
      "my_dict['persuade'] = [u'talk into', u'talk']\n",
      "my_dict['fifteenth'] = ['fifteenth', u'fifteenth']\n",
      "my_dict['achieved'] = ['achieved', 'achieved']\n",
      "my_dict['convince'] = [u'talk into', u'talk']\n",
      "my_dict['diplomats'] = ['diplomats', u'diplomatic']\n",
      "my_dict['poet'] = ['poet', u'poems']\n",
      "my_dict['economies'] = [u'markets', u'market']\n",
      "my_dict['persuaded'] = [u'talked into', u'talk']\n",
      "my_dict['eat'] = ['eat', 'eat']\n",
      "my_dict['holidays'] = ['holidays', 'holidays']\n",
      "my_dict['panda'] = ['panda', 'panda']\n",
      "my_dict['praised'] = ['praised', 'praised']\n",
      "my_dict['eating'] = ['eating', u'eat']\n",
      "my_dict['spheres'] = ['spheres', 'spheres']\n",
      "my_dict['warlords'] = ['warlords', 'warlords']\n",
      "my_dict['continuous'] = ['constant', u'constant']\n",
      "my_dict['philosophies'] = [u'teachings', u'teachings']\n",
      "my_dict['flavor'] = ['taste', u'taste']\n",
      "my_dict['industrialization'] = ['industrialization', 'industrialization']\n",
      "my_dict['dragon'] = ['dragon', 'dragon']\n",
      "my_dict['suffer'] = ['suffer', u'suffering']\n",
      "my_dict['rowed'] = ['rowed', 'rowed']\n",
      "my_dict['foreigners'] = ['outsiders', u'foreign']\n",
      "my_dict['strongest'] = ['strongest', u'strong']\n",
      "my_dict['continuing'] = ['continuing', u'continue']\n",
      "my_dict['lunar'] = [u'moon', u'moon']\n",
      "my_dict['practices'] = ['methods', u'methods']\n",
      "my_dict['festivals'] = ['festivals', 'festivals']\n",
      "my_dict['harvest'] = ['harvest', 'harvest']\n",
      "my_dict['elements'] = ['elements', u'element']\n",
      "my_dict['areas'] = ['areas', u'area']\n",
      "my_dict['statues'] = ['statues', u'statue']\n",
      "my_dict['corpse'] = ['corpse', 'corpse']\n",
      "my_dict['deaths'] = ['deaths', u'death']\n",
      "my_dict['reforms'] = ['reforms', 'reforms']\n",
      "my_dict['enjoy'] = ['enjoy', u'enjoy']\n",
      "my_dict['millennium'] = ['millenium', u'millenium']\n",
      "my_dict['censorship'] = ['censorship', 'censorship']\n",
      "my_dict['n'] = ['n', 'n']\n",
      "my_dict['dumplings'] = ['dumplings', 'dumplings']\n",
      "my_dict['upheaval'] = ['upheaval', 'upheaval']\n",
      "my_dict['nb'] = ['nb', 'nb']\n",
      "my_dict['branched'] = [u'branched', u'branching']\n",
      "my_dict['persian'] = ['persian', 'persian']\n",
      "my_dict['whereas'] = ['where', u'where']\n",
      "my_dict['holding'] = ['holding', u'hold']\n",
      "my_dict['dynasties'] = ['dynasties', u'dynastic']\n",
      "my_dict['feudal'] = ['feudal', u'feudalism']\n",
      "my_dict['chinese'] = ['chinese', u'chinese']\n",
      "my_dict['traditional'] = ['traditional', u'tradition']\n",
      "my_dict['relying'] = [u'depending', u'relies']\n",
      "my_dict['c'] = ['c', 'c']\n",
      "my_dict['rid'] = ['rid', 'rid']\n",
      "my_dict['patriotic'] = ['patriotic', 'patriotic']\n",
      "my_dict['reunion'] = ['reunion', 'reunion']\n",
      "my_dict['sit'] = ['sit', u'sit']\n",
      "my_dict['fled'] = ['fled', 'fled']\n",
      "my_dict['eighth'] = ['eighth', u'eighth']\n",
      "my_dict['richer'] = [u'richer', u'rich']\n",
      "my_dict['rapid'] = ['fast', u'fast']\n",
      "my_dict['leader'] = [u'leader', u'leading']\n",
      "my_dict['emperor'] = ['emperor', u'emperors']\n"
     ]
    }
   ],
   "source": [
    "hold = hold_dict.keys()\n",
    "my = my_dict.keys()\n",
    "for key in my_dict.keys():\n",
    "    if key not in hold:\n",
    "        print \"my_dict['{}'] = {}\".format(key, my_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dict['broadsword'] = ['sword', 'sword']\n",
    "my_dict['uniforms'] = [u'uniforms', u'uniform']\n",
    "my_dict['uniform'] = [u'uniform', u'uniform']\n",
    "my_dict['slashing'] = ['slashing', 'slash']\n",
    "my_dict['stabbing'] = ['stabbing', 'stab']\n",
    "my_dict['wielded'] = ['held', 'hold']\n",
    "my_dict['protecting'] = ['protecting', 'protect']\n",
    "my_dict['cutting'] = ['cutting', 'cut']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    with open('data/basic_english.pickle', 'wb') as handle:\n",
    "         pickle.dump(my_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dict['gods'] = ['gosd', u'god']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'talk into'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-f5eaf0d64a49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmy_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'talk into'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'talk into'"
     ]
    }
   ],
   "source": [
    "\n",
    "my_dict['talk into']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "r = requests.get('https://simple.wikipedia.org/wiki/Alexandre_Dumas,_p%C3%A8re')\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "a = 0\n",
    "ret = ''\n",
    "\n",
    "\n",
    "tags = soup.find_all('p')\n",
    "MyText = '\\n'.join([tag.get_text() for tag in tags])\n",
    "\n",
    "\n",
    "# print tags[tags.index('title=')+6:tags.index('/>')]\n",
    "#tags.span.clear()\n",
    "print MyText\n",
    "#print tags[tags.index('class')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[u'/wiki/Wikipedia:About', u'/wiki/Encyclopedia', u'/wiki/Wikipedia:Introduction', u'/wiki/Special:Statistics', u'/wiki/Wikipedia:Useful', u'/wiki/Special:CategoryTree/Project', u'/wiki/Help:Contents', u'/wiki/Wikipedia:Schools', u'/wiki/Encyclopedia', u'/wiki/Grammar', u'/wiki/Help:Contents', u'/wiki/Wikipedia:Useful', u'/wiki/Vocabulary', u'/wiki/Midget', u'/wiki/Circus', u'/wiki/Uniform', u'/wiki/Carriage', u'/wiki/Walnut', u'/wiki/Democracy', u'/wiki/Hungary', u'/wiki/Execution', u'/wiki/CNN', u'/wiki/T:TDYK#Nominations', u'/wiki/Architecture', u'/wiki/Communication', u'/wiki/Electronics', u'/wiki/Engineering', u'/wiki/Farming', u'/wiki/Health', u'/wiki/Industry', u'/wiki/Medicine', u'/wiki/Transport', u'/wiki/Weather', u'/wiki/Anthropology', u'/wiki/Archaeology', u'/wiki/Geography', u'/wiki/Education', u'/wiki/History', u'/wiki/Language', u'/wiki/Philosophy', u'/wiki/Psychology', u'/wiki/Sociology', u'/wiki/Teaching', u'/wiki/Animation', u'/wiki/Art', u'/wiki/Book', u'/wiki/Cooking', u'/wiki/Custom', u'/wiki/Culture', u'/wiki/Dance', u'/wiki/Family', u'/wiki/Game', u'/wiki/Gardening', u'/wiki/Leisure', u'/wiki/Movie', u'/wiki/Music', u'/wiki/Radio', u'/wiki/Sport', u'/wiki/Theatre', u'/wiki/Travel', u'/wiki/Television', u'/wiki/Algebra', u'/wiki/Astronomy', u'/wiki/Biology', u'/wiki/Chemistry', u'/wiki/Ecology', u'/wiki/Geometry', u'/wiki/Mathematics', u'/wiki/Physics', u'/wiki/Statistics', u'/wiki/Zoology', u'/wiki/Copyright', u'/wiki/Economics', u'/wiki/Government', u'/wiki/Law', u'/wiki/Military', u'/wiki/Politics', u'/wiki/Trade', u'/wiki/Atheism', u'/wiki/Buddhism', u'/wiki/Christianity', u'/wiki/Esotericism', u'/wiki/Hinduism', u'/wiki/Islam', u'/wiki/Jainism', u'/wiki/Judaism', u'/wiki/Mythology', u'/wiki/Paganism', u'/wiki/Sect', u'/wiki/Sikhism', u'/wiki/Taoism', u'/wiki/Theology', u'/wiki/Special:Categories', u'/wiki/Wikipedia:About']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "r = requests.get('https://simple.wikipedia.org/wiki/Main_Page')\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "a = 0\n",
    "ret = ''\n",
    "\n",
    "links = soup.find_all('a')\n",
    "save = []\n",
    "for link in links:\n",
    "    try:\n",
    "        if '/wiki/' in link['href'] and link['title'] in link['href']:\n",
    "            save.append(link['href'])\n",
    "    except:\n",
    "        print ''\n",
    "\n",
    "print save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'/wiki/Wikipedia:About',\n",
       " u'/wiki/Encyclopedia',\n",
       " u'/wiki/Wikipedia:Introduction',\n",
       " u'/wiki/Special:Statistics',\n",
       " u'/wiki/Wikipedia:Useful',\n",
       " u'/wiki/Special:CategoryTree/Project',\n",
       " u'/wiki/Help:Contents',\n",
       " u'/wiki/Wikipedia:Schools',\n",
       " u'/wiki/Encyclopedia',\n",
       " u'/wiki/Grammar',\n",
       " u'/wiki/Help:Contents',\n",
       " u'/wiki/Wikipedia:Useful',\n",
       " u'/wiki/Vocabulary',\n",
       " u'/wiki/Midget',\n",
       " u'/wiki/Circus',\n",
       " u'/wiki/Uniform',\n",
       " u'/wiki/Carriage',\n",
       " u'/wiki/Walnut',\n",
       " u'/wiki/Democracy',\n",
       " u'/wiki/Hungary',\n",
       " u'/wiki/Execution',\n",
       " u'/wiki/CNN',\n",
       " u'/wiki/T:TDYK#Nominations',\n",
       " u'/wiki/Architecture',\n",
       " u'/wiki/Communication',\n",
       " u'/wiki/Electronics',\n",
       " u'/wiki/Engineering',\n",
       " u'/wiki/Farming',\n",
       " u'/wiki/Health',\n",
       " u'/wiki/Industry',\n",
       " u'/wiki/Medicine',\n",
       " u'/wiki/Transport',\n",
       " u'/wiki/Weather',\n",
       " u'/wiki/Anthropology',\n",
       " u'/wiki/Archaeology',\n",
       " u'/wiki/Geography',\n",
       " u'/wiki/Education',\n",
       " u'/wiki/History',\n",
       " u'/wiki/Language',\n",
       " u'/wiki/Philosophy',\n",
       " u'/wiki/Psychology',\n",
       " u'/wiki/Sociology',\n",
       " u'/wiki/Teaching',\n",
       " u'/wiki/Animation',\n",
       " u'/wiki/Art',\n",
       " u'/wiki/Book',\n",
       " u'/wiki/Cooking',\n",
       " u'/wiki/Custom',\n",
       " u'/wiki/Culture',\n",
       " u'/wiki/Dance',\n",
       " u'/wiki/Family',\n",
       " u'/wiki/Game',\n",
       " u'/wiki/Gardening',\n",
       " u'/wiki/Leisure',\n",
       " u'/wiki/Movie',\n",
       " u'/wiki/Music',\n",
       " u'/wiki/Radio',\n",
       " u'/wiki/Sport',\n",
       " u'/wiki/Theatre',\n",
       " u'/wiki/Travel',\n",
       " u'/wiki/Television',\n",
       " u'/wiki/Algebra',\n",
       " u'/wiki/Astronomy',\n",
       " u'/wiki/Biology',\n",
       " u'/wiki/Chemistry',\n",
       " u'/wiki/Ecology',\n",
       " u'/wiki/Geometry',\n",
       " u'/wiki/Mathematics',\n",
       " u'/wiki/Physics',\n",
       " u'/wiki/Statistics',\n",
       " u'/wiki/Zoology',\n",
       " u'/wiki/Copyright',\n",
       " u'/wiki/Economics',\n",
       " u'/wiki/Government',\n",
       " u'/wiki/Law',\n",
       " u'/wiki/Military',\n",
       " u'/wiki/Politics',\n",
       " u'/wiki/Trade',\n",
       " u'/wiki/Atheism',\n",
       " u'/wiki/Buddhism',\n",
       " u'/wiki/Christianity',\n",
       " u'/wiki/Esotericism',\n",
       " u'/wiki/Hinduism',\n",
       " u'/wiki/Islam',\n",
       " u'/wiki/Jainism',\n",
       " u'/wiki/Judaism',\n",
       " u'/wiki/Mythology',\n",
       " u'/wiki/Paganism',\n",
       " u'/wiki/Sect',\n",
       " u'/wiki/Sikhism',\n",
       " u'/wiki/Taoism',\n",
       " u'/wiki/Theology',\n",
       " u'/wiki/Special:Categories',\n",
       " u'/wiki/Wikipedia:About']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "links =  [u'/wiki/Vocabulary',\n",
    " u'/wiki/Democracy',\n",
    " u'/wiki/Execution',\n",
    " u'/wiki/Architecture',\n",
    " u'/wiki/Communication',\n",
    " u'/wiki/Electronics',\n",
    " u'/wiki/Engineering',\n",
    " u'/wiki/Farming',\n",
    " u'/wiki/Health',\n",
    " u'/wiki/Industry',\n",
    " u'/wiki/Medicine',\n",
    " u'/wiki/Transport',\n",
    " u'/wiki/Weather',\n",
    " u'/wiki/Anthropology',\n",
    " u'/wiki/Archaeology',\n",
    " u'/wiki/Geography',\n",
    " u'/wiki/Education',\n",
    " u'/wiki/History',\n",
    " u'/wiki/Language',\n",
    " u'/wiki/Philosophy',\n",
    " u'/wiki/Psychology',\n",
    " u'/wiki/Sociology',\n",
    " u'/wiki/Teaching',\n",
    " u'/wiki/Animation',\n",
    " u'/wiki/Art',\n",
    " u'/wiki/Book',\n",
    " u'/wiki/Cooking',\n",
    " u'/wiki/Custom',\n",
    " u'/wiki/Culture',\n",
    " u'/wiki/Dance',\n",
    " u'/wiki/Family',\n",
    " u'/wiki/Game',\n",
    " u'/wiki/Gardening',\n",
    " u'/wiki/Leisure',\n",
    " u'/wiki/Movie',\n",
    " u'/wiki/Music',\n",
    " u'/wiki/Radio',\n",
    " u'/wiki/Sport',\n",
    " u'/wiki/Theatre',\n",
    " u'/wiki/Travel',\n",
    " u'/wiki/Television',\n",
    " u'/wiki/Algebra',\n",
    " u'/wiki/Astronomy',\n",
    " u'/wiki/Biology',\n",
    " u'/wiki/Chemistry',\n",
    " u'/wiki/Ecology',\n",
    " u'/wiki/Geometry',\n",
    " u'/wiki/Mathematics',\n",
    " u'/wiki/Physics',\n",
    " u'/wiki/Statistics',\n",
    " u'/wiki/Zoology',\n",
    " u'/wiki/Copyright',\n",
    " u'/wiki/Economics',\n",
    " u'/wiki/Government',\n",
    " u'/wiki/Law',\n",
    " u'/wiki/Military',\n",
    " u'/wiki/Politics',\n",
    " u'/wiki/Trade',\n",
    " u'/wiki/Atheism',\n",
    " u'/wiki/Buddhism',\n",
    " u'/wiki/Christianity',\n",
    " u'/wiki/Esotericism',\n",
    " u'/wiki/Hinduism',\n",
    " u'/wiki/Islam',\n",
    " u'/wiki/Jainism',\n",
    " u'/wiki/Judaism',\n",
    " u'/wiki/Mythology',\n",
    " u'/wiki/Paganism',\n",
    " u'/wiki/Sect',\n",
    " u'/wiki/Sikhism',\n",
    " u'/wiki/Taoism',\n",
    " u'/wiki/Theology',\n",
    " u'/wiki/Horse',\n",
    " u'/wiki/France',\n",
    " u'/wiki/French_Revolution',\n",
    " u'/wiki/Sword',\n",
    " u'/wiki/Gun',\n",
    " u'/wiki/War',\n",
    " u'/wiki/Horse',\n",
    " u'/wiki/Alexandre_Dumas,_père'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
